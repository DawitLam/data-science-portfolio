{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde37452",
   "metadata": {},
   "source": [
    "# Medical NLP with Azure - Complete Analysis\n",
    "\n",
    "This notebook demonstrates a comprehensive medical Natural Language Processing pipeline with Azure Cognitive Services integration.\n",
    "\n",
    "## Key Features:\n",
    "- **Medical Text Preprocessing**: Specialized cleaning and normalization for medical texts\n",
    "- **Entity Recognition**: Extract medications, conditions, procedures, and vital signs\n",
    "- **Text Classification**: Classify medical document types (clinical notes, discharge summaries, etc.)\n",
    "- **Sentiment Analysis**: Analyze patient feedback sentiment\n",
    "- **Azure Integration**: Leverage Azure Text Analytics for Health (with local fallbacks)\n",
    "- **Interactive Visualizations**: Comprehensive analysis and reporting\n",
    "\n",
    "## Healthcare Applications:\n",
    "- Clinical documentation analysis\n",
    "- Patient feedback monitoring\n",
    "- Medical record classification\n",
    "- Quality improvement initiatives\n",
    "- Regulatory compliance support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7868ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"ğŸ“š Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb5806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project paths\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to Python path\n",
    "project_root = Path.cwd()\n",
    "src_dir = project_root / 'src'\n",
    "shared_dir = project_root.parent.parent / 'shared'\n",
    "\n",
    "sys.path.insert(0, str(src_dir))\n",
    "sys.path.insert(0, str(shared_dir))\n",
    "\n",
    "print(f\"ğŸ“ Project root: {project_root}\")\n",
    "print(f\"ğŸ“ Source directory: {src_dir}\")\n",
    "print(f\"ğŸ“ Shared directory: {shared_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca82f36",
   "metadata": {},
   "source": [
    "## 1. Data Generation and Loading\n",
    "\n",
    "We'll start by generating synthetic medical text data that mimics real-world clinical documentation while ensuring complete privacy and HIPAA compliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55cd2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules\n",
    "try:\n",
    "    from data_generators.medical_text_generator import MedicalTextGenerator\n",
    "    from nlp_pipeline import MedicalNLPPipeline\n",
    "    print(\"âœ… Custom modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"âš ï¸ Import error: {e}\")\n",
    "    print(\"Installing required packages...\")\n",
    "    \n",
    "    # Install required packages\n",
    "    import subprocess\n",
    "    packages = ['scikit-learn', 'nltk', 'textblob']\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, '-m', 'pip', 'install', package])\n",
    "        except:\n",
    "            print(f\"Failed to install {package}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0faa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic medical text data\n",
    "print(\"ğŸ¥ Generating synthetic medical text data...\")\n",
    "\n",
    "# Initialize generator\n",
    "generator = MedicalTextGenerator(seed=42)\n",
    "\n",
    "# Generate dataset\n",
    "medical_data = generator.generate_dataset(total_records=500)\n",
    "\n",
    "print(f\"âœ… Generated {len(medical_data)} medical text records\")\n",
    "print(f\"ğŸ“Š Columns: {list(medical_data.columns)}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nğŸ“ˆ Note Type Distribution:\")\n",
    "print(medical_data['note_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff9e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample records\n",
    "print(\"ğŸ“ Sample Medical Text Records:\\n\")\n",
    "\n",
    "for note_type in medical_data['note_type'].unique():\n",
    "    sample = medical_data[medical_data['note_type'] == note_type].iloc[0]\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"ğŸ“‹ {note_type.upper().replace('_', ' ')}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Text: {sample['text'][:200]}...\")\n",
    "    if 'sentiment' in sample and pd.notna(sample['sentiment']):\n",
    "        print(f\"Sentiment: {sample['sentiment']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac72bec",
   "metadata": {},
   "source": [
    "## 2. Medical NLP Pipeline Initialization\n",
    "\n",
    "Initialize our comprehensive medical NLP pipeline with all components:\n",
    "- Text preprocessing\n",
    "- Entity recognition\n",
    "- Classification models\n",
    "- Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bca2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the medical NLP pipeline\n",
    "print(\"ğŸ”§ Initializing Medical NLP Pipeline...\")\n",
    "\n",
    "# Configuration for the pipeline\n",
    "config = {\n",
    "    'preserve_medical_terms': True,\n",
    "    'use_azure': False,  # Set to True if you have Azure credentials\n",
    "    'max_features': 3000,\n",
    "    'test_size': 0.2,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = MedicalNLPPipeline()\n",
    "pipeline.config.update(config)\n",
    "\n",
    "print(\"âœ… Pipeline initialized successfully!\")\n",
    "print(f\"ğŸ”¹ Configuration: {pipeline.config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f990ae",
   "metadata": {},
   "source": [
    "## 3. Text Preprocessing Analysis\n",
    "\n",
    "Demonstrate medical-specific text preprocessing including:\n",
    "- Medical abbreviation expansion\n",
    "- Clinical term normalization\n",
    "- Entity-aware tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff12cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run text preprocessing\n",
    "print(\"ğŸ”„ Running text preprocessing...\")\n",
    "\n",
    "processed_data, feature_data = pipeline.preprocess_texts(medical_data)\n",
    "\n",
    "print(f\"âœ… Preprocessed {len(processed_data)} texts\")\n",
    "print(f\"ğŸ“Š Feature matrix shape: {feature_data.shape}\")\n",
    "\n",
    "# Display preprocessing summary\n",
    "prep_summary = pipeline.results['preprocessing_summary']\n",
    "print(\"\\nğŸ“‹ Preprocessing Summary:\")\n",
    "for key, value in prep_summary.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  ğŸ”¹ {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  ğŸ”¹ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82a16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize preprocessing results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Medical Text Preprocessing Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Text length distribution\n",
    "axes[0,0].hist(medical_data['text'].str.len(), bins=30, alpha=0.7, color='skyblue', label='Original')\n",
    "axes[0,0].hist(processed_data['cleaned_text'].str.len(), bins=30, alpha=0.7, color='lightcoral', label='Cleaned')\n",
    "axes[0,0].set_xlabel('Text Length (characters)')\n",
    "axes[0,0].set_ylabel('Frequency')\n",
    "axes[0,0].set_title('Text Length Distribution')\n",
    "axes[0,0].legend()\n",
    "\n",
    "# Token count distribution\n",
    "axes[0,1].hist(processed_data['token_count'], bins=20, alpha=0.7, color='lightgreen')\n",
    "axes[0,1].set_xlabel('Token Count')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "axes[0,1].set_title('Token Count Distribution')\n",
    "\n",
    "# Feature statistics\n",
    "feature_cols = ['char_count', 'word_count', 'sentence_count', 'avg_word_length']\n",
    "feature_stats = feature_data[feature_cols].mean()\n",
    "axes[1,0].bar(range(len(feature_stats)), feature_stats.values, color='gold')\n",
    "axes[1,0].set_xticks(range(len(feature_stats)))\n",
    "axes[1,0].set_xticklabels(feature_stats.index, rotation=45)\n",
    "axes[1,0].set_ylabel('Average Value')\n",
    "axes[1,0].set_title('Average Text Features')\n",
    "\n",
    "# Entity indicators\n",
    "entity_cols = ['medication_count', 'condition_count', 'measurement_count', 'procedure_count']\n",
    "entity_stats = feature_data[entity_cols].mean()\n",
    "axes[1,1].bar(range(len(entity_stats)), entity_stats.values, color='mediumpurple')\n",
    "axes[1,1].set_xticks(range(len(entity_stats)))\n",
    "axes[1,1].set_xticklabels(entity_stats.index, rotation=45)\n",
    "axes[1,1].set_ylabel('Average Count')\n",
    "axes[1,1].set_title('Average Entity Counts')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e4e02a",
   "metadata": {},
   "source": [
    "## 4. Medical Entity Recognition\n",
    "\n",
    "Extract and analyze medical entities including:\n",
    "- Medications and dosages\n",
    "- Medical conditions and diagnoses\n",
    "- Procedures and treatments\n",
    "- Vital signs and measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d877a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run entity recognition\n",
    "print(\"ğŸ” Running medical entity recognition...\")\n",
    "\n",
    "entity_data = pipeline.extract_entities(processed_data)\n",
    "\n",
    "print(f\"âœ… Extracted entities from {len(entity_data)} texts\")\n",
    "\n",
    "# Display entity statistics\n",
    "entity_stats = pipeline.results['entity_statistics']\n",
    "print(\"\\nğŸ“Š Entity Recognition Summary:\")\n",
    "for key, value in entity_stats.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  ğŸ”¹ {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  ğŸ”¹ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32823fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze entity patterns\n",
    "print(\"ğŸ“ˆ Most Common Extracted Entities:\\n\")\n",
    "\n",
    "# Medications\n",
    "all_medications = []\n",
    "for meds in entity_data['medications_found']:\n",
    "    if isinstance(meds, list):\n",
    "        all_medications.extend(meds)\n",
    "    elif isinstance(meds, str) and meds:\n",
    "        all_medications.extend(meds.split(', '))\n",
    "\n",
    "if all_medications:\n",
    "    med_counts = pd.Series(all_medications).value_counts().head(10)\n",
    "    print(\"ğŸ’Š Top Medications:\")\n",
    "    for med, count in med_counts.items():\n",
    "        print(f\"  â€¢ {med}: {count}\")\n",
    "\n",
    "# Conditions\n",
    "all_conditions = []\n",
    "for conds in entity_data['conditions_found']:\n",
    "    if isinstance(conds, list):\n",
    "        all_conditions.extend(conds)\n",
    "    elif isinstance(conds, str) and conds:\n",
    "        all_conditions.extend(conds.split(', '))\n",
    "\n",
    "if all_conditions:\n",
    "    cond_counts = pd.Series(all_conditions).value_counts().head(10)\n",
    "    print(\"\\nğŸ¥ Top Conditions:\")\n",
    "    for cond, count in cond_counts.items():\n",
    "        print(f\"  â€¢ {cond}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9ef825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize entity recognition results\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Entity Distribution by Type', 'Records with Entities', \n",
    "                   'Average Entities per Note Type', 'Entity Co-occurrence'),\n",
    "    specs=[[{'type': 'bar'}, {'type': 'bar'}],\n",
    "           [{'type': 'bar'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# Entity type distribution\n",
    "entity_counts = {\n",
    "    'Medications': entity_stats.get('records_with_medications', 0),\n",
    "    'Conditions': entity_stats.get('records_with_conditions', 0),\n",
    "    'Vital Signs': entity_stats.get('records_with_vital_signs', 0),\n",
    "    'Procedures': entity_stats.get('records_with_procedures', 0)\n",
    "}\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(entity_counts.keys()), y=list(entity_counts.values()),\n",
    "           name='Entity Types', marker_color='lightblue'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Total entities by note type\n",
    "if 'note_type' in processed_data.columns:\n",
    "    entity_by_type = processed_data.groupby('note_type')['token_count'].mean()\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=entity_by_type.index, y=entity_by_type.values,\n",
    "               name='Avg Tokens', marker_color='lightcoral'),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Entity coverage\n",
    "coverage = {\n",
    "    'Has Medications': (entity_data['medications_found'].str.len() > 0).sum(),\n",
    "    'Has Conditions': (entity_data['conditions_found'].str.len() > 0).sum(),\n",
    "    'Has Vital Signs': (entity_data['vital_signs_found'].str.len() > 0).sum(),\n",
    "    'Has Procedures': (entity_data['procedures_found'].str.len() > 0).sum()\n",
    "}\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=list(coverage.keys()), y=list(coverage.values()),\n",
    "           name='Coverage', marker_color='lightgreen'),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Total entities per record\n",
    "total_entities = entity_data['total_entities']\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=total_entities, name='Entity Distribution', \n",
    "                marker_color='gold', nbinsx=20),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=600, title_text=\"Medical Entity Recognition Analysis\", \n",
    "                 showlegend=False)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621e3b4",
   "metadata": {},
   "source": [
    "## 5. Text Classification Models\n",
    "\n",
    "Train and evaluate classification models for:\n",
    "- Medical document type classification\n",
    "- Patient feedback sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ab5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification models\n",
    "print(\"ğŸ¤– Training classification models...\")\n",
    "\n",
    "classification_results = pipeline.train_classification_models(processed_data)\n",
    "\n",
    "print(\"âœ… Model training completed!\")\n",
    "print(\"\\nğŸ“Š Classification Results:\")\n",
    "\n",
    "for task, results in classification_results.items():\n",
    "    print(f\"\\nğŸ”¹ {task.upper()}:\")\n",
    "    for model_name, metrics in results.items():\n",
    "        print(f\"  â€¢ {model_name}:\")\n",
    "        print(f\"    - Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "        print(f\"    - CV Mean Â± Std: {metrics['cv_mean']:.4f} Â± {metrics['cv_std']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168bd5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification performance\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "fig.suptitle('Classification Model Performance', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Note type classification results\n",
    "if 'note_classification' in classification_results:\n",
    "    note_results = classification_results['note_classification']\n",
    "    models = list(note_results.keys())\n",
    "    accuracies = [note_results[model]['test_accuracy'] for model in models]\n",
    "    cv_means = [note_results[model]['cv_mean'] for model in models]\n",
    "    cv_stds = [note_results[model]['cv_std'] for model in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[0].bar(x - width/2, accuracies, width, label='Test Accuracy', alpha=0.8, color='steelblue')\n",
    "    axes[0].errorbar(x + width/2, cv_means, yerr=cv_stds, fmt='o', \n",
    "                    label='CV Mean Â± Std', color='darkred', capsize=5)\n",
    "    \n",
    "    axes[0].set_xlabel('Models')\n",
    "    axes[0].set_ylabel('Accuracy')\n",
    "    axes[0].set_title('Note Type Classification')\n",
    "    axes[0].set_xticks(x)\n",
    "    axes[0].set_xticklabels(models, rotation=45)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sentiment analysis results\n",
    "if 'sentiment_analysis' in classification_results:\n",
    "    sentiment_results = classification_results['sentiment_analysis']\n",
    "    models = list(sentiment_results.keys())\n",
    "    accuracies = [sentiment_results[model]['test_accuracy'] for model in models]\n",
    "    cv_means = [sentiment_results[model]['cv_mean'] for model in models]\n",
    "    cv_stds = [sentiment_results[model]['cv_std'] for model in models]\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    \n",
    "    axes[1].bar(x - width/2, accuracies, width, label='Test Accuracy', alpha=0.8, color='forestgreen')\n",
    "    axes[1].errorbar(x + width/2, cv_means, yerr=cv_stds, fmt='o', \n",
    "                    label='CV Mean Â± Std', color='darkred', capsize=5)\n",
    "    \n",
    "    axes[1].set_xlabel('Models')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].set_title('Sentiment Analysis')\n",
    "    axes[1].set_xticks(x)\n",
    "    axes[1].set_xticklabels(models, rotation=45)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c89817b",
   "metadata": {},
   "source": [
    "## 6. Single Text Analysis Demo\n",
    "\n",
    "Demonstrate real-time analysis of individual medical texts showing the complete pipeline in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba3156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo single text analysis\n",
    "demo_texts = [\n",
    "    \"Patient presents with acute chest pain radiating to left arm. BP 160/95 mmHg, HR 92 bpm. EKG shows ST elevation. Started on aspirin 325mg, nitroglycerin sublingual. Cardiology consulted for urgent catheterization.\",\n",
    "    \n",
    "    \"I am extremely satisfied with the care I received during my hospital stay. The nursing staff was incredibly professional and caring. Dr. Smith took the time to explain my condition thoroughly and answered all my questions patiently. The facility was clean and modern. I would definitely recommend this hospital to others.\",\n",
    "    \n",
    "    \"DISCHARGE SUMMARY: 68-year-old female admitted with acute exacerbation of COPD. Treated with bronchodilators, steroids, and antibiotics. Oxygen saturation improved from 88% to 94% on room air. Patient stable for discharge home with home oxygen therapy and pulmonary rehabilitation referral.\"\n",
    "]\n",
    "\n",
    "print(\"ğŸ” Single Text Analysis Demonstrations\\n\")\n",
    "\n",
    "for i, text in enumerate(demo_texts, 1):\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ğŸ“„ DEMO TEXT {i}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Original Text: {text[:100]}...\")\n",
    "    print()\n",
    "    \n",
    "    # Analyze the text\n",
    "    result = pipeline.analyze_single_text(text)\n",
    "    \n",
    "    # Display preprocessing results\n",
    "    print(\"ğŸ”„ PREPROCESSING:\")\n",
    "    print(f\"  Cleaned: {result['cleaned_text'][:80]}...\")\n",
    "    print(f\"  Tokens: {len(result['tokens'])} tokens\")\n",
    "    print()\n",
    "    \n",
    "    # Display entities\n",
    "    print(f\"ğŸ¥ ENTITIES FOUND ({len(result['entities'])}):\") \n",
    "    entity_groups = {}\n",
    "    for entity in result['entities']:\n",
    "        category = entity['category']\n",
    "        if category not in entity_groups:\n",
    "            entity_groups[category] = []\n",
    "        entity_groups[category].append(entity)\n",
    "    \n",
    "    for category, entities in entity_groups.items():\n",
    "        print(f\"  ğŸ“‹ {category.title()}:\")\n",
    "        for entity in entities[:3]:  # Show top 3 per category\n",
    "            conf_emoji = \"ğŸŸ¢\" if entity['confidence'] > 0.8 else \"ğŸŸ¡\" if entity['confidence'] > 0.5 else \"ğŸŸ \"\n",
    "            print(f\"    {conf_emoji} {entity['text']} (confidence: {entity['confidence']:.2f})\")\n",
    "        if len(entities) > 3:\n",
    "            print(f\"    ... and {len(entities) - 3} more\")\n",
    "    print()\n",
    "    \n",
    "    # Display predictions\n",
    "    if result['predictions']:\n",
    "        print(\"ğŸ¤– PREDICTIONS:\")\n",
    "        for pred_type, pred_data in result['predictions'].items():\n",
    "            conf_emoji = \"ğŸŸ¢\" if pred_data['confidence'] > 0.8 else \"ğŸŸ¡\" if pred_data['confidence'] > 0.5 else \"ğŸŸ \"\n",
    "            print(f\"  {conf_emoji} {pred_type.title()}: {pred_data['prediction']} (confidence: {pred_data['confidence']:.3f})\")\n",
    "            \n",
    "            if 'all_probabilities' in pred_data:\n",
    "                print(f\"    All probabilities: {', '.join([f'{k}: {v:.3f}' for k, v in pred_data['all_probabilities'].items()])}\")\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0edea30",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Analysis Summary\n",
    "\n",
    "Generate a complete analysis report with insights and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27733e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete analysis\n",
    "print(\"ğŸ“Š Running comprehensive analysis...\")\n",
    "\n",
    "complete_results = pipeline.analyze_complete_dataset(medical_data)\n",
    "\n",
    "# Generate and display report\n",
    "report = pipeline.generate_report()\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ“‹ COMPREHENSIVE ANALYSIS REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2104c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dashboard visualization\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=('Note Type Distribution', 'Sentiment Distribution',\n",
    "                   'Entity Coverage by Note Type', 'Text Length Analysis',\n",
    "                   'Model Performance Comparison', 'Processing Pipeline Flow'),\n",
    "    specs=[[{'type': 'pie'}, {'type': 'pie'}],\n",
    "           [{'type': 'bar'}, {'type': 'histogram'}],\n",
    "           [{'type': 'bar'}, {'type': 'bar'}]]\n",
    ")\n",
    "\n",
    "# Note type distribution\n",
    "note_dist = medical_data['note_type'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=note_dist.index, values=note_dist.values, name=\"Note Types\"),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Sentiment distribution (for feedback only)\n",
    "feedback_data = medical_data[medical_data['note_type'] == 'patient_feedback']\n",
    "if 'sentiment' in feedback_data.columns and len(feedback_data) > 0:\n",
    "    sentiment_dist = feedback_data['sentiment'].value_counts()\n",
    "    fig.add_trace(\n",
    "        go.Pie(labels=sentiment_dist.index, values=sentiment_dist.values, name=\"Sentiment\"),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# Entity coverage by note type\n",
    "if 'note_type' in processed_data.columns:\n",
    "    entity_coverage = processed_data.groupby('note_type').agg({\n",
    "        'medications_extracted': lambda x: (x.str.len() > 0).sum(),\n",
    "        'conditions_extracted': lambda x: (x.str.len() > 0).sum(),\n",
    "        'procedures_extracted': lambda x: (x.str.len() > 0).sum()\n",
    "    })\n",
    "    \n",
    "    for i, col in enumerate(['medications_extracted', 'conditions_extracted', 'procedures_extracted']):\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=entity_coverage.index, y=entity_coverage[col], \n",
    "                  name=col.replace('_extracted', '').title(),\n",
    "                  visible=True if i == 0 else 'legendonly'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "\n",
    "# Text length analysis\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=medical_data['text'].str.len(), name='Text Length', nbinsx=30),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Model performance comparison\n",
    "if 'note_classification' in classification_results:\n",
    "    models = list(classification_results['note_classification'].keys())\n",
    "    accuracies = [classification_results['note_classification'][m]['test_accuracy'] for m in models]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=models, y=accuracies, name='Test Accuracy'),\n",
    "        row=3, col=1\n",
    "    )\n",
    "\n",
    "# Pipeline steps\n",
    "pipeline_steps = ['Raw Text', 'Preprocessing', 'Entity Recognition', 'Classification', 'Results']\n",
    "step_counts = [len(medical_data), len(processed_data), len(entity_data), \n",
    "              len(processed_data), len(complete_results['processed_data'])]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(x=pipeline_steps, y=step_counts, name='Records Processed'),\n",
    "    row=3, col=2\n",
    ")\n",
    "\n",
    "fig.update_layout(height=900, title_text=\"Medical NLP Pipeline - Complete Analysis Dashboard\", \n",
    "                 showlegend=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d85592e",
   "metadata": {},
   "source": [
    "## 8. Azure Integration Demo\n",
    "\n",
    "Demonstrate how to integrate with Azure Text Analytics for Health (requires Azure credentials)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c92a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure integration demo\n",
    "print(\"â˜ï¸ Azure Text Analytics Integration Demo\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for Azure credentials\n",
    "import os\n",
    "azure_endpoint = os.getenv('AZURE_TEXT_ANALYTICS_ENDPOINT')\n",
    "azure_key = os.getenv('AZURE_TEXT_ANALYTICS_KEY')\n",
    "\n",
    "if azure_endpoint and azure_key:\n",
    "    print(\"âœ… Azure credentials found!\")\n",
    "    print(f\"ğŸ”— Endpoint: {azure_endpoint[:50]}...\")\n",
    "    \n",
    "    # Enable Azure in pipeline\n",
    "    pipeline.config['use_azure'] = True\n",
    "    pipeline.config['azure_endpoint'] = azure_endpoint\n",
    "    pipeline.config['azure_key'] = azure_key\n",
    "    \n",
    "    # Test with a sample text\n",
    "    test_text = \"Patient prescribed lisinopril 10mg daily for hypertension. Blood pressure 140/90. Follow up in 2 weeks.\"\n",
    "    \n",
    "    print(f\"\\nğŸ” Analyzing with Azure: {test_text}\")\n",
    "    \n",
    "    try:\n",
    "        azure_result = pipeline.analyze_single_text(test_text)\n",
    "        \n",
    "        print(\"\\nâ˜ï¸ Azure Analysis Results:\")\n",
    "        for entity in azure_result['entities']:\n",
    "            print(f\"  â€¢ {entity['text']} ({entity['category']}) [confidence: {entity['confidence']:.2f}]\")\n",
    "            if entity.get('subcategory'):\n",
    "                print(f\"    Subcategory: {entity['subcategory']}\")\n",
    "            if entity.get('normalized_text'):\n",
    "                print(f\"    Normalized: {entity['normalized_text']}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Azure analysis failed: {str(e)}\")\n",
    "        print(\"ğŸ’¡ This might be due to network issues or Azure service limits.\")\n",
    "        \n",
    "else:\n",
    "    print(\"âš ï¸ Azure credentials not found in environment variables.\")\n",
    "    print(\"\\nğŸ’¡ To enable Azure integration:\")\n",
    "    print(\"   1. Set AZURE_TEXT_ANALYTICS_ENDPOINT environment variable\")\n",
    "    print(\"   2. Set AZURE_TEXT_ANALYTICS_KEY environment variable\")\n",
    "    print(\"   3. Restart this notebook\")\n",
    "    print(\"\\nğŸ”§ Using local models instead...\")\n",
    "    \n",
    "    # Demonstrate local analysis\n",
    "    test_text = \"Patient prescribed lisinopril 10mg daily for hypertension. Blood pressure 140/90. Follow up in 2 weeks.\"\n",
    "    local_result = pipeline.analyze_single_text(test_text)\n",
    "    \n",
    "    print(f\"\\nğŸ  Local Analysis Results:\")\n",
    "    for entity in local_result['entities']:\n",
    "        print(f\"  â€¢ {entity['text']} ({entity['category']}) [confidence: {entity['confidence']:.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039c901f",
   "metadata": {},
   "source": [
    "## 9. Production Readiness & Next Steps\n",
    "\n",
    "Summary of capabilities and recommendations for production deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbc648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and results\n",
    "print(\"ğŸ’¾ Saving models and results...\")\n",
    "\n",
    "try:\n",
    "    # Save trained models\n",
    "    pipeline.save_models('models')\n",
    "    print(\"âœ… Models saved to 'models/' directory\")\n",
    "    \n",
    "    # Save analysis results\n",
    "    pipeline.save_results('outputs')\n",
    "    print(\"âœ… Results saved to 'outputs/' directory\")\n",
    "    \n",
    "    print(\"\\nğŸ“ Generated Files:\")\n",
    "    import os\n",
    "    \n",
    "    if os.path.exists('models'):\n",
    "        model_files = os.listdir('models')\n",
    "        print(\"  ğŸ¤– Models:\")\n",
    "        for file in model_files:\n",
    "            print(f\"    â€¢ {file}\")\n",
    "    \n",
    "    if os.path.exists('outputs'):\n",
    "        output_files = os.listdir('outputs')\n",
    "        print(\"  ğŸ“Š Outputs:\")\n",
    "        for file in output_files:\n",
    "            print(f\"    â€¢ {file}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error saving files: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production readiness summary\n",
    "print(\"ğŸš€ PRODUCTION READINESS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "capabilities = [\n",
    "    \"âœ… Medical text preprocessing with domain-specific normalization\",\n",
    "    \"âœ… Multi-class document classification (clinical notes, discharge summaries, etc.)\",\n",
    "    \"âœ… Medical entity recognition (medications, conditions, procedures, vitals)\",\n",
    "    \"âœ… Sentiment analysis for patient feedback\",\n",
    "    \"âœ… Azure Text Analytics for Health integration with local fallbacks\",\n",
    "    \"âœ… Configurable pipeline with YAML/JSON configuration\",\n",
    "    \"âœ… Model persistence and loading capabilities\",\n",
    "    \"âœ… Comprehensive analysis reporting\",\n",
    "    \"âœ… HIPAA-compliant synthetic data generation\",\n",
    "    \"âœ… Interactive visualization and analysis\"\n",
    "]\n",
    "\n",
    "print(\"\\nğŸ¯ Key Capabilities:\")\n",
    "for capability in capabilities:\n",
    "    print(f\"  {capability}\")\n",
    "\n",
    "print(\"\\nğŸ¥ Healthcare Use Cases:\")\n",
    "use_cases = [\n",
    "    \"ğŸ“‹ Clinical documentation analysis and quality assurance\",\n",
    "    \"ğŸ’¬ Patient feedback sentiment monitoring and analysis\",\n",
    "    \"ğŸ·ï¸ Automated medical record classification and routing\",\n",
    "    \"ğŸ” Medical entity extraction for research and analytics\",\n",
    "    \"ğŸ“Š Healthcare quality improvement initiatives\",\n",
    "    \"âš¡ Real-time clinical decision support systems\",\n",
    "    \"ğŸ“ˆ Population health analytics and insights\",\n",
    "    \"ğŸ›¡ï¸ Regulatory compliance and audit support\"\n",
    "]\n",
    "\n",
    "for use_case in use_cases:\n",
    "    print(f\"  {use_case}\")\n",
    "\n",
    "print(\"\\nğŸ”§ Deployment Recommendations:\")\n",
    "recommendations = [\n",
    "    \"ğŸŒ Deploy as REST API using FastAPI or Flask\",\n",
    "    \"ğŸ³ Containerize with Docker for scalable deployment\",\n",
    "    \"â˜ï¸ Use Azure Container Instances or Kubernetes for cloud deployment\",\n",
    "    \"ğŸ”’ Implement proper authentication and authorization\",\n",
    "    \"ğŸ“Š Add monitoring and logging for production operations\",\n",
    "    \"ğŸ”„ Set up CI/CD pipeline for model updates\",\n",
    "    \"ğŸ§ª Implement A/B testing for model improvements\",\n",
    "    \"ğŸ“š Create comprehensive API documentation\"\n",
    "]\n",
    "\n",
    "for rec in recommendations:\n",
    "    print(f\"  {rec}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ‰ Medical NLP Pipeline Analysis Complete!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
